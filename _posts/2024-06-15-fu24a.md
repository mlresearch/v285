---
title: Conic Activation Functions
booktitle: 'UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural
  Models'
year: '2024'
url: https://openreview.net/forum?id=6EDbuqER4p
abstract: Most activation functions operate component-wise, which restricts the equivariance
  of neural networks to permutations. We introduce Conic Linear Units (CoLU) and generalize
  the symmetry of neural networks to continuous orthogonal groups. By interpreting
  ReLU as a projection onto its invariant set-the positive orthant-we propose a conic
  activation function that uses a Lorentz cone instead. Its performance can be further
  improved by considering multi-head structures, soft scaling, and axis sharing. CoLU
  associated with low-dimensional cones outperforms the component-wise ReLU in a wide
  range of models-including MLP, ResNet, and UNet, etc., achieving better loss values
  and faster convergence. It significantly improves diffusion modelsâ€™ training and
  performance. CoLU originates from a first-principles approach to various forms of
  neural networks and fundamentally changes their algebraic structure.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fu24a
month: 0
tex_title: Conic Activation Functions
firstpage: 296
lastpage: 309
page: 296-309
order: 296
cycles: false
bibtex_author: Fu, Changqing and Cohen, Laurent D.
author:
- given: Changqing
  family: Fu
- given: Laurent D.
  family: Cohen
date: 2024-06-15
address:
container-title: 'Proceedings of UniReps: the Second Edition of the Workshop on Unifying
  Representations in Neural Models'
volume: '285'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 6
  - 15
pdf: https://raw.githubusercontent.com/mlresearch/v285/main/assets/fu24a/fu24a.pdf
extras:
- label: Supplementary PDF
  link: https://raw.githubusercontent.com/mlresearch/v285/main/assets/assets/fu24a/fu24a-supp.pdf
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
