@Proceedings{unireps24,
booktitle={Proceedings of UniReps: the Second Edition of the Workshop on Unifying Representations in Neural Models},
shortname={UniReps},
year={2024},
editor={Fumero, Marco and Domine, Clementine and L{\"a}hner, Zorah and Crisostomi, Donato and Moschella, Luca and Stachenfeld, Kimberly},
volume={285}, 
start={2024-12-14},
end={2024-12-14},
published = {2024-06-15},
address={Vancouver Convention Center,Vancouver, Canada},
conference_url={https://unireps.org}
}
@inproceedings{
domine24,
title={Preface of UniReps: the Second Edition of the Workshop on Unifying Representations in Neural Models},
author={Domine, Clementine and Fumero, Marco and L{\"a}hner, Zorah and Crisostomi, Donato and Moschella, Luca and Stachenfeld, Kimberly},
booktitle={UniReps: the Second Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://unireps.org},
abstract={Discover why, when and how distinct learning processes yield similar representations, and the degree to which these can be unified.},
pages={1-9}
}
@inproceedings{
williams24,
title={Equivalence between representational similarity analysis, centered kernel alignment, and canonical correlations analysis},
author={Williams, Alex H},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=zMdnnFasgC},
abstract={Centered kernel alignment (CKA) and representational similarity analysis (RSA) of dissimilarity matrices are two popular methods for quantifying similarity in neural representational geometry. Although they follow a conceptually similar approach, typical implementations of CKA and RSA tend to result in numerically different outcomes. Here, I show that these two approaches are largely equivalent once one incorporates a mean-centering step into RSA. This connection is quite simple to derive, but appears to have been thus far overlooked by the community studying neural representational geometry. By unifying these measures, this paper hopes to simplify a complex and fragmented literature on this subject.},
pages={10-23}
}
@inproceedings{
alvarez24,
title={Decision-margin consistency: a principled metric for human and machine performance alignment},
author={Alvarez, George A. and Konkle, Talia},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=y2FPllMQVg},
abstract={Understanding the alignment between human and machine perceptual decision-making is a fundamental challenge. While most current vision deep neural networks are deterministic and produce consistent outputs for the same input, human perceptual decisions are notoriously noisy. This noise can originate from perceptual encoding, decision processes, or even attentional fluctuations, leading to different responses for the same stimulus across trials. Thus, any meaningful comparison between human-to-human or human-to-machine decisions must take this internal noise into account to avoid underestimating alignment. In this paper, we introduce the \textbf{decision-margin consistency metric}, which draws on signal detection theory, by incorporating both the variability in decision difficulty across items and the noise in human responses. By focusing on decision-margin distances-continuous measures of signal strength underlying binary outcomes-our method can be applied to both model and human systems to capture the nuanced agreement in item-level difficulty. Applying this metric to existing visual categorization datasets reveals a dramatic increase in human-human agreement relative to the standard error consistency metric. Further, human-to-machine agreement showed only a modest increase, highlighting an even larger representational gap between these systems on these challenging perceptual decisions. Broadly, this work underscores the importance of accounting for internal noise when comparing human and machine error patterns, and offers a new principled metric for measuring representational alignment for biological and artificial systems},
pages={24-39}
}
@inproceedings{
han24,
title={Investigating the role of modality and training objective on representational alignment between transformers and the brain},
author={Han, Hyewon Willow and Dhar, Ruchira and Yang, Qingqing and Behbahani, Maryam Hoseini and Ortiz, Mar{\'i}a Alejandra Mart{\'i}nez and Oladele, Tolulope Samuel and Dima, Diana C and Li, Hsin-Hung and S{\o}gaard, Anders and Mohsenzadeh, Yalda},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=t4CnKu6yXn},
abstract={The remarkable performance of transformer models in both linguistic and real-world reasoning tasks coupled with their ubiquitous use has prompted much research on their alignment with brain activations. However, there remain some unanswered questions: what aspects of these models lead to representational alignment- the input modality or the training objective? Moreover, is the alignment limited to modality-specialized brain regions, or can representations align with brain regions involved in higher cognitive functions? To address this, we analyze the representations of different transformer architectures, including text-based and vision-based language models, and compare them with neural representations across multiple brain regions obtained during a visual processing task. Our findings reveal that both training data modality and training objective are important in determining alignment, and that models align with neural representations within and beyond the modality-specific regions. Additionally, the training modality and objectives seem to have an impact on alignment quality as we progress through the layers, suggesting that multimodal data along with a predictive processing objective may confer superior representational capabilities compared to other training objectives.},
pages={40-54}
}
@inproceedings{
protani24,
title={Federated {GNN}s for {EEG}-Based Stroke Assessment},
author={Protani, Andrea and Giusti, Lorenzo and Aillet, Albert Sund and Sacco, Simona and Manganotti, Paolo and Marinelli, Lucio and Santos, Diogo Reis and Brutti, Pierpaolo and Caliandro, Pietro and Serio, Luigi},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=swSFdpTzqK},
abstract={Machine learning (ML) has the potential to become an essential tool in supporting clinical decision-making processes, offering enhanced diagnostic capabilities and personalized treatment plans. However, outsourcing medical records to train ML models using patient data raises legal, privacy, and security concerns. Federated learning has emerged as a promising paradigm for collaborative ML, meeting healthcare institutions' requirements for robust models without sharing sensitive data and compromising patient privacy. This study proposes a novel method that combines federated learning (FL) and Graph Neural Networks (GNNs) to predict stroke severity using electroencephalography (EEG) signals across multiple medical institutions. Our approach enables multiple hospitals to jointly train a shared GNN model on their local EEG data without exchanging patient information.  Specifically, we address a regression problem by predicting the National Institutes of Health Stroke Scale (NIHSS), a key indicator of stroke severity. The proposed model leverages a masked self-attention mechanism to capture salient brain connectivity patterns and employs EdgeSHAP to provide post-hoc explanations of the neurological states after a stroke. We evaluated our method on EEG recordings from four institutions, achieving a mean absolute error (MAE) of 3.23 in predicting NIHSS, close to the average error made by human experts (MAE $\approx$ 3.0). This demonstrates the method's effectiveness in providing accurate and explainable predictions while maintaining data privacy.},
pages={55-68}
}
@inproceedings{
small24,
title={Vision and language representations in multimodal {AI} models and human social brain regions during natural movie viewing},
author={Small, Hannah and Masson, Haemy Lee and Mostofsky, Stewart and Isik, Leyla},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=pS1UjuYuJu},
abstract={Recent work in NeuroAI suggests that representations in modern AI vision and language models are highly aligned with each other and human visual cortex. In addition, training AI vision models on language-aligned tasks (e.g., CLIP-style models) improves their match to visual cortex, particularly in regions involved in social perception, suggesting these brain regions may be similarly "language aligned". This prior work has primarily investigated only static stimuli without language, but in our daily lives, we experience the dynamic visual world and communicate about it using language simultaneously. To understand the processing of vision and language during natural viewing, we fit an encoding model to predict voxel-wise responses to an audiovisual movie using visual representations from both purely visual and language-aligned vision transformer models and paired language transformers. We first find that in naturalistic settings, there is remarkably low correlation between representations in vision and language models and both predict social perceptual and language regions well. Next, we find that language-alignment does not improve a vision model embedding's match to neural responses in social perceptual regions, despite these regions being well predicted by both vision and language embeddings. Preliminary analyses, however, suggest that vision-alignment does improve a language model's ability to match neural responses in language regions during audiovisual processing.  Our work demonstrates the importance of testing multimodal AI models in naturalistic settings and reveals differences between language alignment in modern AI models and the human brain.},
pages={69-84}
}
@inproceedings{
hong24,
title={Debiasing Global Workspace: A Cognitive Neural Framework for Learning Debiased and Interpretable Representations},
author={Hong, Jinyung and Jeon, Eun Som and Kim, Changhoon and Park, Keun Hee and Nath, Utkarsh and Yang, Yezhou and Turaga, Pavan K. and Pavlic, Theodore P.},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=obiwUsWlki},
abstract={When trained on biased datasets, Deep Neural Networks (DNNs) often make predictions based on attributes derived from features spuriously correlated with the target labels. This is especially problematic if these irrelevant features are easier for the model to learn than the truly relevant ones. Many existing approaches, called debiasing methods, have been proposed to address this issue, but they often require predefined bias labels and entail significantly increased computational complexity by incorporating extra auxiliary models. Instead, we provide an orthogonal perspective from the existing approaches, inspired by cognitive science, specifically Global Workspace Theory (GWT). Our method, Debiasing Global Workspace (DGW), is a novel debiasing framework that consists of specialized modules and a shared workspace, allowing for increased modularity and improved debiasing performance. Additionally, DGW enhances the transparency of decision-making processes by visualizing which features of the inputs the model focuses on during training and inference through attention masks.  We begin by proposing an instantiation of GWT for the debiasing method. We then outline the implementation of each component within DGW. At the end, we validate our method across various biased datasets, proving its effectiveness in mitigating biases and improving model performance.},
pages={85-99}
}
@inproceedings{
alleman24,
title={Unsupervised Learning of Categorical Structure},
author={Alleman, Matteo and Fusi, Stefano},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=oXdVhIAWtA},
abstract={Humans occasionally reason using logic and abstract categories, and yet most state of the art neural models use continuous distributed representations. These representations are impressive in their learning capabilities, but have proven difficult to interpret, or to compare to biological representations. But continuous representations can sometimes be interpreted symbolically, and a distributed code can seem to be constructed by composing abstract categories. We ask whether it is possible to detect and get back this structure, and we answer that it sort of is. The demixing problem is equivalent to factorizing the data into a continuous and a binary part $\mathbf{X}= \mathbf{W}\mathbf{S}^T$. After establishing some general facts and intuitions, we present two algorithms which work on low-rank or full-rank data, assess their reliability on extensive simulated data, and use them to interpret neural word embeddings where we expect some compositional structure. We hope this problem is interesting and that our simple algorithms provide a promising direction for solving it.},
pages={100-114}
}
@inproceedings{
kashyap24,
title={Modern Hopfield Networks meet Encoded Neural Representations - Addressing Practical Considerations},
author={Kashyap, Satyananda and D'Souza, Niharika S. and Shi, Luyao and Wong, Ken C. L. and Wang, Hongzhi and Syeda-mahmood, Tanveer},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=jpP2Umzsru},
abstract={Content-addressable memories such as Modern Hopfield Networks (MHN) have been studied as mathematical models of the auto-association and storage/retrieval in the human declarative memory, yet their practical use for large-scale content storage faces challenges. Chief among them is the occurrence of meta-stable states, particularly when handling large amounts of high dimensional content. This paper introduces Hopfield Encoding Networks (HEN), a framework that integrates encoded neural representations into MHNs to improve pattern separability and reduce meta-stable states. We show that HEN can also be used for retrieval in the context of hetero association of images with natural language queries, thus removing the limitation of requiring access to partial content in the same domain. Experimental results demonstrate substantial reduction in meta-stable states and increased storage capacity while still enabling perfect recall of a significantly larger number of inputs advancing the practical utility of associative memory networks for real-world tasks.},
pages={115-127}
}
@inproceedings{
zieba24,
title={Hypernetworks for image recontextualization},
author={Zieba, Maciej and Balicki, Jakub and Drozdz, Tomasz and Karanowski, Konrad and Lorek, Pawel and Lyu, Hong and Skorupa, Aleksander Piotr and Trzcinski, Tomasz and Caudevilla, Oriol and Tomczak, Jakub M.},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=jV7jT05qgv},
abstract={Image recontextualization, the task of placing a subject from an image into a new context to serve a specific purpose, has become increasingly important in fields like art, media, marketing, and e-commerce. Recent advancements in deep generative modeling, such as text-to-image and image-to-image synthesis via diffusion models, have significantly improved recontextualization capabilities. However, current methods, like DreamBooth and LoRA, require time-consuming fine-tuning per individual image, resulting in inefficiencies and often suboptimal outputs. Other approaches to recontextualization, like MagicClothing, require reorganization of the architecture of the base model and a time-consuming training process in a particular domain. In this work, we propose HyperLoRA, a novel framework that leverages hypernetworks to predict LoRA parameters, allowing for more efficient image recontextualization without the need for image-specific fine-tuning. HyperLoRA utilizes domain pairs of context images and target objects, enabling instant adaptation to new contexts while significantly reducing computational costs. Our method outperforms traditional techniques by offering more accurate adjustments, broader applicability across multiple modalities (e.g., text, video, sound, and structured data), and scalable deployment. Experimental results demonstrate the effectiveness of our approach in garment-to-model recontextualization, highlighting the potential for broader applications.},
pages={128-139}
}
@inproceedings{
harvey24,
title={What Representational Similarity Measures Imply about Decodable Information},
author={Harvey, Sarah E and Lipshutz, David and Williams, Alex H},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=hqfzH6GCYj},
abstract={Neural responses encode information that is useful for a variety of downstream tasks. A common approach to understand these systems is to build regression models or decoders that reconstruct features of the stimulus from neural responses. Here, we investigate how to leverage this perspective to quantify the similarity of different neural systems. This is distinct from typical motivations behind neural network similarity measures like centered kernel alignment (CKA), canonical correlation analysis (CCA), and Procrustes shape distance, which highlight geometric intuition and invariances to orthogonal or affine transformations. We show that CKA, CCA, and other measures can be equivalently motivated from similarity in decoding patterns. Specifically, these measures quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. We also show that the Procrustes shape distance upper bounds the distance between optimal linear readouts and that the converse holds for representations with low participation ratio. Overall, our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures.},
pages={140-151}
}
@inproceedings{
meshkinnejad24,
title={Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks},
author={Meshkinnejad, Rouzbeh and Mei, Jie and Zhang, Zeduo and Lizotte, Daniel J and Mohsenzadeh, Yalda},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=fS41j9Ksds},
abstract={Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in task-incremental, class-incremental, and domain-incremental continual learning scenarios.},
pages={152-169}
}
@inproceedings{
li24,
title={Hybrid Dynamic High-Order Functional Correlations and Divisive Normalization for Improved Classification of Schizophrenia and Bipolar Disorder},
author={Li, Qiang and Calhoun, Vince},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=bbBb26IWLG},
abstract={Schizophrenia and bipolar disorder are devastating psychiatric disorders that can be difficult to adequately classify, considering commonalities that make it difficult to distinguish between them using conventional classification approaches based on low-order functional connectivity. Recently, high-order functional connectivity has emerged as a promising method for diagnosing psychiatric illnesses, and this research applied multiple strategies for distinguishing schizophrenia and bipolar disorder using features taken from dynamic high-order functional connectivity and divisive normalization. The approach that produced the greatest results combined dynamic high-order functional correlations and divisive normalization to examine patterns of intrinsic connection time courses collected from resting-state fMRI. Our findings indicate that resting-state fMRI-based dynamic high-order functional connectivity and feature enhancement through divisive normalization classification hold significant promise for improving the accuracy of psychiatric diagnoses. Moreover, to the best of our knowledge, this study is the first to integrate divisive normalization with functional connectivity in fMRI.},
pages={170-180}
}
@inproceedings{
orme24,
title={Correlating Variational Autoencoders Natively For Multi-View Imputation},
author={Orme, Ella S C and Evangelou, Marina and Paquet, Ulrich},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=T6IbERflun},
abstract={Multi-view data from the same source often exhibit correlation. This is mirrored in correlation between the latent spaces of separate variational autoencoders (VAEs) trained on each data-view. A multi-view VAE approach is proposed that incorporates a joint prior with a non-zero correlation structure between the latent spaces of the VAEs. By enforcing such correlation structure, more strongly correlated latent spaces are uncovered. Using conditional distributions to move between these latent spaces, missing views can be imputed and used for downstream analysis. Learning this correlation structure involves maintaining validity of the prior distribution, as well as a successful parameterization that allows end-to-end learning.},
pages={181-193}
}
@inproceedings{
rothermel24,
title={On the cognitive alignment between humans and machines},
author={Rothermel, Marco and Daftarian, Sayed Soroush and Koosha, Tahmineh A. and Mahani, Mohammad-Ali Nikouei and Jamalabadi, Hamidreza},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=SkLYMkxE6n},
abstract={In this paper, we explore the psychological relevance, similarity to brain representations, and subject-invariance of latent space representations in generative models. Using fMRI data from four subjects who viewed over 9,000 visual stimuli, we conducted three experiments to investigate this alignment. First, we assessed whether a linear mapping between the latent space of a generative mode, in this case a very deep VAE (VDVAE), and fMRI brain responses could accurately capture cognitive properties, specifically emotional valence, of the visual stimuli presented to both humans and machines. Second, we examined whether perturbing psychologically relevant dimensions in either the generative model or human brain data would produce corresponding cognitive effects in both systems --- across models and human subjects. Third, we investigated whether a nonlinear mapping, approximated via a Taylor expansion up to the fifth degree, would outperform linear mapping in aligning cognitive properties. Our findings revealed three key insights: (1) the latent space of the generative model aligns with fMRI brain responses across all subjects tested (r ~ 0.4, (2) perturbations in the psychologically relevant dimensions of both the fMRI data and the generative model resulted in highly consistent effects across the aligned systems (both the model and human subjects), and (3) a linear mapping, approximated using Ridge regression, performed as well as or better than all Taylor expansions we tested. Together, these results suggest a universal cognitive alignment between humans and between human-model systems. This universality holds significant potential for advancing our understanding of basic cognitive processes and offers promising new avenues for studying mental disorders.},
pages={194-203}
}
@inproceedings{
ackbari24,
title={Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with {EEG}-Vision Transformer},
author={Akbari, Ali and Arani, Kosar Sanjar and Yousefnezhad, Tony and Mirian, Maryam and Arasteh, Emad},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=SjXJOsLi1X},
abstract={Reconstructing visual stimuli from brain activity is a challenging problem, particularly when using EEG data, which is more affordable and accessible than fMRI but noisier and lower in spatial resolution. In this paper, we present Hierarchical-ViT, a novel framework designed to improve the quality and precision of EEG-based image reconstruction by integrating hierarchical visual feature extraction, vision transformer-based EEG (EEG-ViT) processing, and CLIP-based joint learning. Inspired by the hierarchical nature of the human visual system, our model progressively captures complex visual features-such as edges, textures, and shapes-through a multi-stage processing approach. These features are aligned with EEG signals processed by the EEG-ViT model, allowing for the creation of a shared latent space that enhances contrastive learning. A StyleGAN is then employed to generate high-resolution images from these aligned representations. We evaluated our method on two benchmark datasets, EEGCVPR40 and ThoughtViz, achieving superior results compared to existing approaches in terms of Inception Score (IS), Kernel Inception Distance (KID), and Frchet Inception Distance (FID) for EEGCVPR, and IS and KID for the ThoughtViz dataset. Through an ablation study, we underscored the feasibility of hierarchical feature extraction, while multivariate analysis of variance (MANOVA) test confirmed the distinctiveness of the learned feature spaces. 
In conclusion, our results show the feasibility and uniqueness of using hierarchical filtering of perceived images combined with EEG-ViT-based features to improve brain decoding from EEG data.},
pages={204-218}
}
@inproceedings{
garcia-castellanos25,
title={Relative Representations: Topological and Geometric Perspectives},
author={Garc{\'i}a-Castellanos, Alejandro and Marchetti, Giovanni Luca and Kragic, Danica and Scolamiero, Martina},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=RDfkKNoET5},
abstract={Relative representations are an established approach to zero-shot model stitching, consisting of a non-trainable transformation of the latent space of a deep neural network. Based on insights of topological and geometric nature, we propose two improvements to relative representations. First, we introduce a normalization procedure in the relative transformation, resulting in invariance to non-isotropic rescalings and permutations. The latter coincides with the symmetries in parameter space induced by common activation functions. Second, we propose to deploy topological densification when fine-tuning relative representations, a topological regularization loss encouraging clustering within classes. We provide an empirical investigation on a natural language task, where both the proposed variations yield improved performance on zero-shot model stitching.},
pages={219-231}
}
@inproceedings{
zhang24,
title={Fast Imagic: Solving Overfitting in Text-guided Image Editing via Disentangled {UN}et with Forgetting Mechanism and Unified Vision-Language Optimization},
author={Zhang, Shiwen},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=PoLsUIDY0c},
abstract={Text-guided image editing on real or synthetic images, given only the original image itself and the target text prompt as inputs, is a very general and challenging task. It requires an editing model to estimate by itself which part of the image should be edited, and then perform  either rigid or non-rigid editing while preserving the characteristics of original image.   Imagic, the previous SOTA solution to text-guided image editing, suffers from slow optimization speed, and is prone to overfitting since there is only one image given.  In this paper, we  design a novel text-guided image editing method, Fast Imagic. First, we propose a vision-language joint optimization framework for fast aligning text embedding and UNet with the given image, which is capable of understanding and reconstructing the original image in 30 seconds, much faster and much less overfitting than  previous SOTA Imagic.  Then we propose a novel vector projection mechanism in text embedding space of Diffusion Models,  capable of  decomposing the identity similarity and editing strength thus controlling them separately. Finally, we discovered a general disentanglement property of UNet in Diffusion Models, i.e., UNet encoder learns space and structure, UNet decoder learns appearance and texture. With such a property, we design the forgetting mechanism by merging original checkpoint and optimized checkpoint to successfully tackle the fatal and inevitable overfitting issues when fine-tuning Diffusion Models on one image, thus significantly boosting the editing capability of Diffusion Models. Our method, Fast Imagic, even built on the outdated Stable Diffusion, achieves new state-of-the-art results on the challenging text-guided image editing benchmark: TEdBench,  surpassing the previous SOTA methods such as Imagic with Imagen, in terms of both CLIP score and LPIPS score.},
pages={232-243}
}
@inproceedings{
kalibhat24,
title={Disentangling the Effects of Data Augmentation and Format Transform in Self-Supervised Learning of Image Representations},
author={Kalibhat, Neha and Morningstar, Warren Richard and Bijamov, Alex and Liu, Luyang and Singhal, Karan and Mansfield, Philip Andrew},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=PA0UxaGptd},
abstract={Self-Supervised Learning (SSL) enables training performant models using limited labeled data. One of the pillars underlying vision SSL is the use of data augmentations-perturbations of the input which do not significantly alter its semantic content. For audio and other temporal signals, augmentations are commonly used alongside format transforms such as Fourier transforms or wavelet transforms. Unlike augmentations, format transforms do not change the information contained in the data; rather, they express the same information in different coordinates. In this paper, we study the effects of format transforms and augmentations both separately and together on vision SSL. We define augmentations in frequency space called Fourier Domain Augmentations (FDA) and show that training SSL models on a combination of these and image augmentations can improve the downstream classification accuracy by up to 1.3\% on ImageNet-1K. We also show improvements against SSL baselines in few-shot and transfer learning setups using FDA. Surprisingly, we also observe that format transforms can improve the quality of learned representations even without augmentations; however, the combination of the two techniques yields better quality.},
pages={244-256}
}
@inproceedings{
dwivedi24,
title={Representation Learning of Structured Data for Medical Foundation Models},
author={Dwivedi, Vijay Prakash and Schlegel, Viktor and Liu, Andy T. and Nguyen, Thanh-Tung and Kashyap, Abhinav Ramesh and Wei, Jeng and Yin, Wei-Hsian and Winkler, Stefan and Tan, Robby T.},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=Lu3Y2f54g9},
abstract={Large Language Models (LLMs) have demonstrated remarkable performance across various domains, including healthcare. However, their ability to effectively represent structured non-textual data, such as the alphanumeric medical codes used in records like ICD-10 or SNOMED-CT, is limited and has been particularly exposed in recent research. This paper examines the challenges LLMs face in processing medical codes due to the shortcomings of current tokenization methods. As a result, we introduce the UniStruct architecture to design a multimodal medical foundation model of unstructured text and structured data, which addresses these challenges by adapting subword tokenization techniques specifically for the structured medical codes. Our approach is validated through model pre-training on both an extensive internal medical database and a public repository of structured medical records. Trained on over 1 billion tokens on the internal medical database, the proposed model achieves up to a 23\% improvement in evaluation metrics, with around 2\% gain attributed to our proposed tokenization. Additionally, when evaluated on the EHRSHOT public benchmark with a 1/1000 fraction of the pre-training data, the UniStruct model improves performance on over 42\% of the downstream tasks. Our approach not only enhances the representation and generalization capabilities of patient-centric models but also bridges a critical gap in representation learning models ability to handle complex structured medical data, alongside unstructured text.},
pages={257-268}
}
@inproceedings{
joglekar24,
title={Adapter to facilitate Foundation Model Communication for {DLO} Instance Segmentation},
author={Joglekar, Omkar and Kozlovsky, Shir and Castro, Dotan Di},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=EYBtzTQvdW},
abstract={Classical methods in Digital Communication rely on mixing transmitted signals with carrier frequencies to eliminate signal distortion through noisy channels. Drawing inspiration from these techniques, we present an adapter network that enables CLIPSeg, a text-conditioned semantic segmentation model, to communicate point prompts to the Segment Anything Model (SAM) in the positional embedding space. We showcase our technique on the complex task of Deformable Linear Object (DLO) Instance Segmentation. Our method combines the strong zero-shot generalization capability of SAM and user-friendliness of CLIPSeg to exceed the SOTA performance in DLO Instance Segmentation in terms of DICE Score, while training only 0.7\% of the model parameters.},
pages={269-281}
}
@inproceedings{
karimi24,
title={Comparing Representations in Static and Dynamic Vision Models to the Human Brain},
author={Karimi, Hamed and Anzellotti, Stefano},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=9iJhYQ2EBj},
abstract={We compared neural responses to naturalistic videos and representations in deep network models trained with static and dynamic information. Models trained with dynamic information showed greater correspondence with neural representations in all brain regions, including those previously associated with the processing of static information. Among the models trained with dynamic information, those based on optic flow accounted for unique variance in neural responses that were not captured by Masked Autoencoders. This effect was strongest in ventral and dorsal brain regions, indicating that despite the Masked Autoencoders' effectiveness at a variety of tasks, their representations diverge from representations in the human brain in the early stages of visual processing.},
pages={282-295}
}
@inproceedings{
fu24,
title={Conic Activation Functions},
author={Fu, Changqing and Cohen, Laurent D.},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=6EDbuqER4p},
abstract={Most activation functions operate component-wise, which restricts the equivariance of neural networks to permutations. We introduce Conic Linear Units (CoLU) and generalize the symmetry of neural networks to continuous orthogonal groups. By interpreting ReLU as a projection onto its invariant set-the positive orthant-we propose a conic activation function that uses a Lorentz cone instead. Its performance can be further improved by considering multi-head structures, soft scaling, and axis sharing. CoLU associated with low-dimensional cones outperforms the component-wise ReLU in a wide range of models-including MLP, ResNet, and UNet, etc., achieving better loss values and faster convergence. It significantly improves diffusion models' training and performance. CoLU originates from a first-principles approach to various forms of neural networks and fundamentally changes their algebraic structure.},
pages={296-309}
}
@inproceedings{
mcneal24,
title={Small-scale adversarial perturbations expose differences between predictive encoding models of human f{MRI} responses},
author={McNeal, Nikolas and Deb, Mainak and Murty, Apurva Ratan},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=1g7HWFmvY0},
abstract={Artificial neural network-based vision encoding models have made significant strides in predicting neural responses and providing insights into visual cognition. However, progress appears to be slowing, with many encoding models achieving similar levels of accuracy in predicting brain activity. In this study, we show that encoding models of human fMRI responses are highly vulnerable to small-scale adversarial attacks, revealing differences not captured using predictive accuracy alone. We then test adversarial sensitivity as a complementary evaluation measure and show that it offers a more effective way to distinguish between highly predictive encoding models. While explicit adversarial training can increase robustness of encoding models, we find that it comes at the cost of brain prediction accuracy. Our preliminary findings also indicate that the choice of model features-to-brain mapping might play a role in optimizing both robustness and accuracy, with sparse mappings typically resulting in more robust encoding models of neural activity. These findings reveal key vulnerabilities of current models, introduce a novel evaluation procedure, and offer a path toward improving the balance between robustness and predictive accuracy for future encoding models.},
pages={310-323}
}
@inproceedings{
zheng24,
title={Delays in generalization match delayed changes in representational geometry},
author={Zheng, Xingyu and Daruwalla, Kyle and Benjamin, Ari S and Klindt, David},
booktitle={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
year={2024},
url={https://openreview.net/forum?id=1ae108kHk2},
abstract={Delayed generalization, also known as "grokking", has emerged as a well-replicated phenomenon in overparameterized neural networks. Recent theoretical works associated grokking with the transition from lazy to rich learning regime, measured as the change in the Neural Tangent Kernel (NTK) from its initial state. Here, we present an empirical study on image classification tasks. Surprisingly, we demonstrate that the NTK deviates from its initial state significantly before the onset of grokking, i.e., before test performance increases, suggesting that rich learning does occur before generalization. To explain this difference, we instead look at the representational geometry of the network, and find that grokking coincides in time with a rapid increase in manifold capacity and improved effective geometry metrics. Notably, this sharp transition is absent when generalization is not delayed. Our findings on real data show that lazy and rich training regimes can become decoupled from sudden generalization. In contrast, changes in representational geometry remain tightly linked and may therefore better explain grokking dynamics.},
pages={324-334}
}
